# Canvassing

This evaluation tests a modelâ€™s ability to conduct an end-to-end canvassing call with a voter in a federal election.

The headline metric is the number of votes the canvasser earned after the calls.

## Usage

Run with:

```
oaieval <canvasser>,<canvasee>,<judge> canvassing
```

We suggest using `gpt-4-32k` or `gpt-3.5-turbo-16k` as a default `<canvasser>` ; and `gpt-4-32k` as a default `<canvassee>` and `<judge>`.

## Evaluation Process



## Prompts

See `defaults.py` to review/adjust the prompts used in this eval.

## Dataset


## Metrics

## Variants

## Token Usage Estimates

No estimates are currently available.

## Version History

- v0: Initial version released

## Contribution Statement
